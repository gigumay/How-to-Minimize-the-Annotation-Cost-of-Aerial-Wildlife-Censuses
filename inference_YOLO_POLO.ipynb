{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46d4c92",
   "metadata": {},
   "source": [
    "### Load dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e66fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.inference_YOLO import *\n",
    "from utils.model_eval import *\n",
    "from sahi.predict import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c82e2",
   "metadata": {},
   "source": [
    "### Insert paths, output directories, patch dimensions etc.\n",
    "- We use 640x640 pathces in the paper. \n",
    "- The amount of overlap can be calculated from the values specified in Table 2. E.g.: $\\frac{128}{640} = 0.2$\n",
    "- The DoR and IoU values can be found in Table 12.\n",
    "- The radii can be found in the Size Avg. column of Table 1. They need to be passed as a dictionary where `key = class ID`, `value = radius`. The ID of a species is its position in the Species column of Table 1. E.g., for the JE-TL19 dataset, the `radii` dict would look as follows: `{0: 62, 1: 81, 2: 49}`\n",
    "- `classID2name` is a dictionary mapping class ids to their name. Again, for the JE-TL19 dataset: `classID2name = {0: \"Elephant\", 1: \"Giraffe\", 2: \"Zebra\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_dir = \"\"   #TODO: Insert Path to image directory (as downloaded from Zenodo).\n",
    "output_dir = \"\"     #TODO: Insert path to directory you want your output stored in\n",
    "patch_dims = {\"width\": -1, \"height\": -1}  #TODO: Insert patch width and height; (640, 640) in the paper\n",
    "ovrlp = -1.0    #TODO: Define patch overlap as a fraction of the patch dimensions (e.g. 128 pixel overlap = 0.2 of 640x640 patches; cf. Table 2)\n",
    "dor_thresh = -1.0    #TODO: Define DoR threshold for NMS and computing evaluation metrics (cf. Table 12)\n",
    "iou_thresh = -1.0    #TODO: Define IoU threshold for NMS and computing evaluation metrics (cf. Table 12)\n",
    "mdl_path = \"\"     #TODO: Insert path to the model .pt file\n",
    "radii = {0: None, 1: None, 2: None}     #TODO: Insert radius for each species in the dataset; cf. Table 1.\n",
    "classID2name = {0: \"\", 1: \"\", 2: \"\"}    #TODO: Insert class ids and names as listed in Table 1.\n",
    "img_format = \"\"     #TODO: Specify the image format (suffix). E.g.: \"JPG\"/\"jpg\" (case sensitive)\n",
    "device = \"\"     #TODO: Insert device to be used for inference. \"cuda\" if you have access to a GPU, \"cpu\" otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d35c8",
   "metadata": {},
   "source": [
    "### Set some more paths (no input required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f94151",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_file = f\"{imgs_dir}/test_annotations.json\"\n",
    "tiling_dir = f\"{imgs_dir}/tiles\"\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e35b19",
   "metadata": {},
   "source": [
    "### Define Task\n",
    "Set this variable to `\"locate\"` if you are working with a POLO model, use `\"detect\"` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"locate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbc734",
   "metadata": {},
   "source": [
    "### Run tiled inference (no input required)\n",
    "For bounding box models, we use the `SAHI` library. `coco_file_path` will point to a json file required by `SAHI` to run tiled inference for bounding box models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == \"locate\":\n",
    "    run_tiled_inference_POLO(model=mdl_path, \n",
    "                             class_ids=list(radii.keys()),\n",
    "                             imgs_dir=imgs_dir, \n",
    "                             img_files_ext=img_format,\n",
    "                             patch_dims=patch_dims, \n",
    "                             patch_overlap=ovrlp, \n",
    "                             output_dir=output_dir,\n",
    "                             dor_thresh=dor_thresh,\n",
    "                             radii=radii,\n",
    "                             ann_file=ann_file,\n",
    "                             ann_format=\"PT_DEFAULT\")\n",
    "else:\n",
    "    coco_file_path = [p for p in Path(imgs_dir).glob(\"*.json\") if \"coco\" in p][0]\n",
    "    predict(\n",
    "        model_type=\"yolov8\",\n",
    "        model_path=mdl_path,\n",
    "        model_device=device, \n",
    "        source=imgs_dir,\n",
    "        slice_height=patch_dims[\"height\"],\n",
    "        slice_width=patch_dims[\"width\"],\n",
    "        overlap_height_ratio=ovrlp,\n",
    "        overlap_width_ratio=ovrlp,\n",
    "        postprocess_match_threshold=iou_thresh,\n",
    "        dataset_json_path=coco_file_path,\n",
    "        project=output_dir, \n",
    "        name=\"output_SAHI\",\n",
    "        novisual=True, \n",
    "        verbose=0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757e7cd",
   "metadata": {},
   "source": [
    "### Compute Evaluation metrics\n",
    "Executing this cells will generate a number of outputs:\n",
    "- `count_diffs_img_lvl.xlsx`: Excel sheet containing the difference between predicted and ground truth count for each image.\n",
    "- `counts_gt_pred_*.png`: Plot of predicted vs. forund truth count for class `*`.\n",
    "- `counts_total.json`: Predicted counts summed over all images.\n",
    "- `em.json`: Evaluation metrics.\n",
    "- `errors_img_lvl.json`: Counting metrics.\n",
    "- `F1_curve.png`: F1 score plotted against the confidence threshold.\n",
    "- `P_curve.png`: Precision plotted against the confidence threshold.\n",
    "- `R_curve.png`: Recall plotted against the confidence threshold.\n",
    "\n",
    "Before running the cell, please set the `is_pseudo` variable to `True` if you are using a `YOLOv8` model trained on pseudo boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == \"detect\":\n",
    "    is_pseudo = None    #TODO: Set to True if you are using a YOLOv8 model trained on pseudo boxes\n",
    "\n",
    "    box_dims = {cid: {\"width\": radii[cid], \"height\": radii[cid]} for cid in radii.keys()} if is_pseudo else None\n",
    "    read_output_SAHI(out_json_SAHI=f\"{output_dir}/output_SAHI/result.json\", dataset_json_SAHI=coco_file_path, class_ids=list(classID2name.keys()), \n",
    "                         iou_thresh=iou_thresh, box_dims=box_dims, ann_file=ann_file, ann_format=\"BX_WH\", output_dir=output_dir)\n",
    "\n",
    "    compute_errors_img_lvl(gt_counts_dir=f\"{imgs_dir}/image_counts\", pred_counts_dir=f\"{output_dir}/detections\", class_ids=list(classID2name.keys()), \n",
    "                           output_dir=output_dir)\n",
    "    compute_em_img_lvl(preds_dir=f\"{output_dir}/detections\", class_id2name=classID2name, task=task, output_dir=output_dir)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P0_YOLOcate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
