{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46d4c92",
   "metadata": {},
   "source": [
    "### Load dependencies \n",
    "We start by importing all necessary python packages, and the functionalities implemented in the utils folder of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e66fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from utils.inference_POLO import *\n",
    "from utils.model_eval import *\n",
    "from utils.processing_utils import *\n",
    "from sahi.predict import predict\n",
    "from utils.data_params import parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c82e2",
   "metadata": {},
   "source": [
    "### Insert dataset name, output directories, etc. (**TODO**)\n",
    "Here we specify the dataset we want to test, and set the some paths. For the dataset, please use the abbreviations given in the paper. E.g., `data_set = BK-L23`. This will load the corresponding IoU-/DoR-threshold, patch overlap, etc. in the next cell (values can also be found in the paper). Please set the following variables:\n",
    "- `data_set`: Name (abbreviation of the dataset).\n",
    "- `imgs_dir`: Path to the directory containing the test images of the dataset.\n",
    "- `output_dir`: Path to the directory ourtputs can be stored in (must exist, is NOT going to be created).\n",
    "- `mdl_path`: Path to the trained model file (.pt).\n",
    "- `is_pseudo`: Set to true if you are using a YOLOv8_p model.\n",
    "- `device`: Device on which to load and run the model. E.g., `\"0\"`. Pass `\"cpu\"` if you do not have a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = \"\"\n",
    "imgs_dir = \"\"\n",
    "output_dir = \"\" \n",
    "mdl_path = \"\"\n",
    "is_pseudo = False\n",
    "device = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ddb059",
   "metadata": {},
   "source": [
    "For this next cell, no input is required - just run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70617370",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dims = {\"width\": 640, \"height\": 640}  \n",
    "ovrlp = parameters[data_set][\"ovrlp\"]   \n",
    "dor_thresh = parameters[data_set][\"dor_thresh\"]   \n",
    "iou_thresh = parameters[data_set][\"iou_thresh\"] if not is_pseudo else  parameters[data_set][\"iou_thresh_pseudo\"]\n",
    "radii = parameters[data_set][\"radii\"]\n",
    "classID2name = parameters[data_set][\"classID2name\"]\n",
    "img_format = parameters[data_set][\"img_format\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d35c8",
   "metadata": {},
   "source": [
    "### Set some more paths (no input required)\n",
    "Here the path to the file containing the test set annotations, and to the tiling folder are set. The annotations file is needed to compute evaluation metrics and counting errors after inference, and the tiling folder is where the patches extracted from each image are going to be stored. We also set the random seed for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f94151",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_file = f\"{imgs_dir}/test_annotations.json\"\n",
    "tiling_dir = f\"{imgs_dir}/tiles\"\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e35b19",
   "metadata": {},
   "source": [
    "### Define Task (**TODO**)\n",
    "Here we specify what model we will be using. Set the `task` variable to `\"locate\"` if you are working with a POLO model, use `\"detect\"` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd9980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"detect\"     #TODO: define task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbc734",
   "metadata": {},
   "source": [
    "### Run tiled inference (no input required)\n",
    "This is where we run the actual inference For bounding box models, we use the `SAHI` library, for POLO we use the methods implemented in `utils/inference_POLO.py`. `coco_file_path` will point to a json file required by `SAHI` to run tiled inference for bounding box models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: skipping file /home/giacomo/data/test_sets/EW-IL22/test_annotations.json as the file extension suggests that it's not a picture!\n",
      "WARNING: skipping file /home/giacomo/data/test_sets/EW-IL22/image_counts as the file extension suggests that it's not a picture!\n",
      "WARNING: skipping file /home/giacomo/data/test_sets/EW-IL22/counts_total.json as the file extension suggests that it's not a picture!\n",
      "WARNING: skipping file /home/giacomo/data/test_sets/EW-IL22/EW-IL22_coco_dataset.json as the file extension suggests that it's not a picture!\n"
     ]
    }
   ],
   "source": [
    "if task == \"locate\":\n",
    "    run_tiled_inference_POLO(model=mdl_path, \n",
    "                             class_ids=list(radii.keys()),\n",
    "                             imgs_dir=imgs_dir, \n",
    "                             img_files_ext=img_format,\n",
    "                             patch_dims=patch_dims, \n",
    "                             patch_overlap=ovrlp, \n",
    "                             output_dir=output_dir,\n",
    "                             dor_thresh=dor_thresh,\n",
    "                             radii=radii,\n",
    "                             ann_file=ann_file,\n",
    "                             ann_format=parameters[data_set][\"ann_format\"],\n",
    "                             box_dims=parameters[data_set][\"bx_dims\"])\n",
    "else:\n",
    "    categories = [{\"id\": k, \"name\": v} for k,v in classID2name.items()]\n",
    "    coco_file_path = make_coco_file(imgs_dir=imgs_dir, categories=categories)\n",
    "    predict(\n",
    "        model_type=\"yolov8\",\n",
    "        model_path=mdl_path,\n",
    "        model_device=f\"cuda:{device}\", \n",
    "        source=imgs_dir,\n",
    "        slice_height=patch_dims[\"height\"],\n",
    "        slice_width=patch_dims[\"width\"],\n",
    "        overlap_height_ratio=ovrlp,\n",
    "        overlap_width_ratio=ovrlp,\n",
    "        postprocess_match_threshold=iou_thresh,\n",
    "        postprocess_type=\"NMS\" if data_set == \"JE-TL19\" else \"GREEDYNMM\",\n",
    "        postprocess_match_metric=\"IOU\"  if data_set == \"JE-TL19\" else \"IOS\",\n",
    "        dataset_json_path=coco_file_path,\n",
    "        project=output_dir, \n",
    "        name=\"output_SAHI\",\n",
    "        novisual=True, \n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757e7cd",
   "metadata": {},
   "source": [
    "### Compute Evaluation metrics (no input required)\n",
    "This cell evaluated the ouputs of the previous cell. It will generate a number of files:\n",
    "- `count_diffs_img_lvl.xlsx`: Excel sheet containing the difference between predicted and ground truth count for each image.\n",
    "- `counts_gt_pred_*.png`: Plot of predicted vs. forund truth count for class `*`.\n",
    "- `counts_total.json`: Predicted counts summed over all images.\n",
    "- `em.json`: Evaluation metrics.\n",
    "- `errors_img_lvl.json`: Counting metrics.\n",
    "- `F1_curve.png`: F1 score plotted against the confidence threshold.\n",
    "- `P_curve.png`: Precision plotted against the confidence threshold.\n",
    "- `R_curve.png`: Recall plotted against the confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giacomo/projects/How-to-Minimize-the-Annotation-Cost-of-Aerial-Wildlife-Censuses/utils/model_eval.py:95: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  count_diffs_df = pd.concat([count_diffs_df, row_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'confusion': 1.8992034087562226,\n",
       "  'precision': 90.91321063500939,\n",
       "  'recall': 96.02711382718752,\n",
       "  'f1': 93.40021426044356,\n",
       "  'metrics/precision(B)': 0.9427717845289977,\n",
       "  'metrics/recall(B)': 0.9225932316869347,\n",
       "  'metrics/mAP50(B)': 0.9602139655670826,\n",
       "  'metrics/mAP50-95(B)': 0.7075851231836722},\n",
       " 1: {'confusion': 11.088295708170604,\n",
       "  'precision': 66.6723695352143,\n",
       "  'recall': 79.36863542171719,\n",
       "  'f1': 72.46861873716462,\n",
       "  'metrics/precision(B)': 0.7566046929826677,\n",
       "  'metrics/recall(B)': 0.7431771894093686,\n",
       "  'metrics/mAP50(B)': 0.7132288613041664,\n",
       "  'metrics/mAP50-95(B)': 0.4921976047765605},\n",
       " 2: {'confusion': 10.325047972609847,\n",
       "  'precision': 52.69662915427345,\n",
       "  'recall': 80.30821904056812,\n",
       "  'f1': 63.63636307156705,\n",
       "  'metrics/precision(B)': 0.6590481117270406,\n",
       "  'metrics/recall(B)': 0.791095890410959,\n",
       "  'metrics/mAP50(B)': 0.6951550416040699,\n",
       "  'metrics/mAP50-95(B)': 0.12310158872538937},\n",
       " 3: {'confusion': 4.730251985665445,\n",
       "  'precision': 86.75394142922946,\n",
       "  'recall': 85.20669154082584,\n",
       "  'f1': 85.97335514125226,\n",
       "  'metrics/precision(B)': 0.9224520778268913,\n",
       "  'metrics/recall(B)': 0.8222903423187605,\n",
       "  'metrics/mAP50(B)': 0.887494015272821,\n",
       "  'metrics/mAP50-95(B)': 0.6472347788274201},\n",
       " 4: {'confusion': 22.794118214749126,\n",
       "  'precision': 62.87425112051347,\n",
       "  'recall': 46.66666645925926,\n",
       "  'f1': 53.57142780905093,\n",
       "  'metrics/precision(B)': 0.7904759224647682,\n",
       "  'metrics/recall(B)': 0.4444444444444444,\n",
       "  'metrics/mAP50(B)': 0.6052933816946379,\n",
       "  'metrics/mAP50-95(B)': 0.42220137247934975},\n",
       " 'binary': {'confusion': 10.16738345799025,\n",
       "  'precision': 90.65503161765356,\n",
       "  'recall': 96.2847887869864,\n",
       "  'f1': 93.38513866330992,\n",
       "  'metrics/precision(B)': 0.8142705179060732,\n",
       "  'metrics/recall(B)': 0.7447202196540935,\n",
       "  'metrics/mAP50(B)': 0.7722770530885554,\n",
       "  'metrics/mAP50-95(B)': 0.4784640935984783}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if task == \"detect\":\n",
    "    read_output_SAHI(out_json_SAHI=f\"{output_dir}/output_SAHI/result.json\", dataset_json_SAHI=coco_file_path, class_ids=list(classID2name.keys()), \n",
    "                     iou_thresh=iou_thresh, ann_file=ann_file, ann_format=parameters[data_set][\"ann_format\"], box_dims=parameters[data_set][\"bx_dims\"], output_dir=output_dir)\n",
    "\n",
    "compute_errors_img_lvl(gt_counts_dir=f\"{imgs_dir}/image_counts\", pred_counts_dir=f\"{output_dir}/detections\", class_ids=list(classID2name.keys()), \n",
    "                           output_dir=output_dir)\n",
    "compute_em_img_lvl(preds_dir=f\"{output_dir}/detections\", class_id2name=classID2name, task=task, output_dir=output_dir)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P0_YOLOcate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
