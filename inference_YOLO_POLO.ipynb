{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46d4c92",
   "metadata": {},
   "source": [
    "### Load dependencies \n",
    "We start by importing all necessary python packages, and the functionalities implemented in the utils folder of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e66fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from utils.inference_POLO import *\n",
    "from utils.model_eval import *\n",
    "from utils.processing_utils import *\n",
    "from sahi.predict import predict\n",
    "from utils.data_params import parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c82e2",
   "metadata": {},
   "source": [
    "### Insert paths, output directories, patch dimensions etc.\n",
    "Here we specify the inference parameters, including which model and test set to use, the dimensions of the patches, their overlap, etc. Some general info:\n",
    "- We use 640x640 patches in the paper. \n",
    "- The amount of overlap can be calculated from the values specified in Table 2. E.g.: $\\frac{128}{640} = 0.2$\n",
    "- The DoR and IoU values can be found in Table 12.\n",
    "- The radii can be found in the Size Avg. column of Table 1. They need to be passed as a dictionary where `key = class ID`, `value = radius`. The ID of a species is its position in the Species column of Table 1. E.g., for the JE-TL19 dataset, the `radii` dict would look as follows: `{0: 62, 1: 81, 2: 49}`\n",
    "- `classID2name` is a dictionary mapping class ids to their name. Again, for the JE-TL19 dataset: `classID2name = {0: \"Elephant\", 1: \"Giraffe\", 2: \"Zebra\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164b874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = \"EW-IL22\"\n",
    "imgs_dir = \"/home/giacomo/data/test_sets/EW-IL22\"   #TODO: Insert Path to image directory (as downloaded from Zenodo).\n",
    "output_dir = \"/home/giacomo/projects/How-to-Minimize-the-Annotation-Cost-of-Aerial-Wildlife-Censuses/outputs/EW-IL22/YOLOv8n_p\"     #TODO: Insert path to directory you want your output stored in\n",
    "mdl_path = \"/home/giacomo/data/weights/EW-IL22/yolov8n_p_EW-IL22.pt\"     #TODO: Insert path to the model .pt file\n",
    "is_pseudo = True       #TODO: Set to True if working with a pseudo model\n",
    "device = \"0\"     #TODO: Insert device to be used for inference. \"cuda\" if you have access to a GPU, \"cpu\" otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70617370",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dims = {\"width\": 640, \"height\": 640}  \n",
    "ovrlp = parameters[data_set][\"ovrlp\"]   \n",
    "dor_thresh = parameters[data_set][\"dor_thresh\"]   \n",
    "iou_thresh = parameters[data_set][\"iou_thresh\"] if not is_pseudo else  parameters[data_set][\"iou_thresh_pseudo\"]\n",
    "radii = parameters[data_set][\"radii\"]\n",
    "classID2name = parameters[data_set][\"classID2name\"]\n",
    "img_format = parameters[data_set][\"img_format\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d35c8",
   "metadata": {},
   "source": [
    "### Set some more paths (no input required)\n",
    "Here the path to the file containing the test set annotations, and to the tiling folder are set. The annotations file is needed to compute evaluation metrics and counting errors after inference, and the tiling folder is where the patches extracted from each image are going to be stored. We also set the random seed for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f94151",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_file = f\"{imgs_dir}/test_annotations.json\"\n",
    "tiling_dir = f\"{imgs_dir}/tiles\"\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e35b19",
   "metadata": {},
   "source": [
    "### Define Task\n",
    "Here we specify what model we will be using. Set the `task` variable to `\"locate\"` if you are working with a POLO model, use `\"detect\"` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd9980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"detect\"     #TODO: define task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbc734",
   "metadata": {},
   "source": [
    "### Run tiled inference (no input required)\n",
    "This is where we run the actual inference For bounding box models, we use the `SAHI` library, for POLO we use the methods implemented in `utils/inference_POLO.py`. `coco_file_path` will point to a json file required by `SAHI` to run tiled inference for bounding box models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d32ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: skipping file /home/giacomo/data/test_sets/EW-IL22/test_annotations.json as the file extension suggests that it's not a picture!\n",
      "WARNING: skipping file /home/giacomo/data/test_sets/EW-IL22/image_counts as the file extension suggests that it's not a picture!\n",
      "WARNING: skipping file /home/giacomo/data/test_sets/EW-IL22/counts_total.json as the file extension suggests that it's not a picture!\n",
      "WARNING: skipping file /home/giacomo/data/test_sets/EW-IL22/EW-IL22_coco_dataset.json as the file extension suggests that it's not a picture!\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 748/748 [00:00<00:00, 796885.80it/s]\n",
      "Performing inference on images: 100%|██████████| 748/748 [37:25<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results are successfully exported to /home/giacomo/projects/How-to-Minimize-the-Annotation-Cost-of-Aerial-Wildlife-Censuses/outputs/EW-IL22/YOLOv8n_p/output_SAHI\n"
     ]
    }
   ],
   "source": [
    "if task == \"locate\":\n",
    "    bx_dims = {cid: {\"width\": radii[cid], \"height\": radii[cid]} for cid in radii.keys()} if data_set == \"EW-IL22\" else None\n",
    "    run_tiled_inference_POLO(model=mdl_path, \n",
    "                             class_ids=list(radii.keys()),\n",
    "                             imgs_dir=imgs_dir, \n",
    "                             img_files_ext=img_format,\n",
    "                             patch_dims=patch_dims, \n",
    "                             patch_overlap=ovrlp, \n",
    "                             output_dir=output_dir,\n",
    "                             dor_thresh=dor_thresh,\n",
    "                             radii=radii,\n",
    "                             ann_file=ann_file,\n",
    "                             ann_format=\"BX_WH\",\n",
    "                             box_dims=bx_dims)\n",
    "else:\n",
    "    categories = [{\"id\": k, \"name\": v} for k,v in classID2name.items()]\n",
    "    coco_file_path = make_coco_file(imgs_dir=imgs_dir, categories=categories)\n",
    "\n",
    "    predict(\n",
    "        model_type=\"yolov8\",\n",
    "        model_path=mdl_path,\n",
    "        model_device=f\"cuda:{device}\", \n",
    "        source=imgs_dir,\n",
    "        slice_height=patch_dims[\"height\"],\n",
    "        slice_width=patch_dims[\"width\"],\n",
    "        overlap_height_ratio=ovrlp,\n",
    "        overlap_width_ratio=ovrlp,\n",
    "        postprocess_match_threshold=iou_thresh,\n",
    "        postprocess_type=\"NMS\" if data_set == \"JE-TL19\" else \"GREEDYNMM\",\n",
    "        postprocess_match_metric=\"IOU\"  if data_set == \"JE-TL19\" else \"IOS\",\n",
    "        dataset_json_path=coco_file_path,\n",
    "        project=output_dir, \n",
    "        name=\"output_SAHI\",\n",
    "        novisual=True, \n",
    "        verbose=0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757e7cd",
   "metadata": {},
   "source": [
    "### Compute Evaluation metrics (no input required)\n",
    "This cell evaluated the ouputs of the previous cell. It will generate a number of files:\n",
    "- `count_diffs_img_lvl.xlsx`: Excel sheet containing the difference between predicted and ground truth count for each image.\n",
    "- `counts_gt_pred_*.png`: Plot of predicted vs. forund truth count for class `*`.\n",
    "- `counts_total.json`: Predicted counts summed over all images.\n",
    "- `em.json`: Evaluation metrics.\n",
    "- `errors_img_lvl.json`: Counting metrics.\n",
    "- `F1_curve.png`: F1 score plotted against the confidence threshold.\n",
    "- `P_curve.png`: Precision plotted against the confidence threshold.\n",
    "- `R_curve.png`: Recall plotted against the confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giacomo/projects/How-to-Minimize-the-Annotation-Cost-of-Aerial-Wildlife-Censuses/utils/model_eval.py:95: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  count_diffs_df = pd.concat([count_diffs_df, row_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'confusion': 1.9581443025915912,\n",
       "  'precision': 74.27545069352118,\n",
       "  'recall': 51.72009140800877,\n",
       "  'f1': 60.97887281845156,\n",
       "  'metrics/precision(B)': 0.5706254913610201,\n",
       "  'metrics/recall(B)': 0.4676139494459104,\n",
       "  'metrics/mAP50(B)': 0.46977220426815275,\n",
       "  'metrics/mAP50-95(B)': 0.08319039976647617},\n",
       " 1: {'confusion': 14.415094404215035,\n",
       "  'precision': 44.78672984013162,\n",
       "  'recall': 23.095723009552806,\n",
       "  'f1': 30.475678123888617,\n",
       "  'metrics/precision(B)': 0.24782543629956552,\n",
       "  'metrics/recall(B)': 0.20814663951120163,\n",
       "  'metrics/mAP50(B)': 0.13745838445591418,\n",
       "  'metrics/mAP50-95(B)': 0.028411932501594933},\n",
       " 2: {'confusion': 12.979351289146457,\n",
       "  'precision': 44.696969629247015,\n",
       "  'recall': 50.51369854364093,\n",
       "  'f1': 47.42765215873492,\n",
       "  'metrics/precision(B)': 0.4098975858010135,\n",
       "  'metrics/recall(B)': 0.4931506849315068,\n",
       "  'metrics/mAP50(B)': 0.3054284652313954,\n",
       "  'metrics/mAP50-95(B)': 0.07388097993809636},\n",
       " 3: {'confusion': 4.6496609930094746,\n",
       "  'precision': 82.11902111175779,\n",
       "  'recall': 40.82676620477993,\n",
       "  'f1': 54.538738118679404,\n",
       "  'metrics/precision(B)': 0.4847667890601736,\n",
       "  'metrics/recall(B)': 0.376061295629502,\n",
       "  'metrics/mAP50(B)': 0.3292434916862888,\n",
       "  'metrics/mAP50-95(B)': 0.06802009848256438},\n",
       " 4: {'confusion': 22.22222318244169,\n",
       "  'precision': 47.014925022276685,\n",
       "  'recall': 27.99999987555556,\n",
       "  'f1': 35.097492372809036,\n",
       "  'metrics/precision(B)': 0.4114475238505281,\n",
       "  'metrics/recall(B)': 0.2311111111111111,\n",
       "  'metrics/mAP50(B)': 0.24745145762735013,\n",
       "  'metrics/mAP50-95(B)': 0.08025041158481468},\n",
       " 'binary': {'confusion': 11.244894834280851,\n",
       "  'precision': 74.46274017085426,\n",
       "  'recall': 49.49902152577301,\n",
       "  'f1': 59.46725342403414,\n",
       "  'metrics/precision(B)': 0.4249125652744602,\n",
       "  'metrics/recall(B)': 0.35521673612584637,\n",
       "  'metrics/mAP50(B)': 0.29787080065382027,\n",
       "  'metrics/mAP50-95(B)': 0.0667507644547093}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if task == \"detect\":\n",
    "    bx_dims = {cid: {\"width\": 50, \"height\": 50} for cid in radii.keys()} if data_set == \"EW-IL22\" else None\n",
    "    read_output_SAHI(out_json_SAHI=f\"{output_dir}/output_SAHI/result.json\", dataset_json_SAHI=coco_file_path, class_ids=list(classID2name.keys()), \n",
    "                     iou_thresh=iou_thresh, ann_file=ann_file, ann_format=\"BX_WH\", box_dims=bx_dims, output_dir=output_dir)\n",
    "\n",
    "compute_errors_img_lvl(gt_counts_dir=f\"{imgs_dir}/image_counts\", pred_counts_dir=f\"{output_dir}/detections\", class_ids=list(classID2name.keys()), \n",
    "                           output_dir=output_dir)\n",
    "compute_em_img_lvl(preds_dir=f\"{output_dir}/detections\", class_id2name=classID2name, task=task, output_dir=output_dir)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P0_YOLOcate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
